# Introduction
This repository lists papers on incorporating inductive, symbolic, and linguistic biases into neural network-based architectures to better align with human behavior in language learning, generalization, targeted evaluation, and sentence processing. Contributions are welcome—feel free to open an issue or submit a PR!

# Learning & Generalization
| **Paper**                                                    | **Venue/Journal** | **Year** | **Code**                                               | 
| ------------------------------------------------------------ | --------- | -------- | ------------------------------------------------------------ |
| [Strengthening Structural Inductive Biases by Pre-training to Perform Syntactic Transformations](https://aclanthology.org/2024.emnlp-main.645/) | EMNLP | 2024| [code](https://github.com/namednil/step)|
|[SIP: Injecting a Structural Inductive Bias into a Seq2Seq Model by Simulation](https://aclanthology.org/2024.acl-long.355/) | ACL| 2024 | [code](https://github.com/namednil/sip)|
|[Injecting structural hints: Using language models to study inductive biases in language learning](https://aclanthology.org/2023.findings-emnlp.563/) | EMNLP findings | 2023| [code](https://github.com/toizzy/injecting-structural-hints)|
|[Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs](https://arxiv.org/abs/2309.07311) | ICLR| 2024| NA|
|[Does Vision Accelerate Hierarchical Generalization in Neural Language Learners?](https://aclanthology.org/2025.coling-main.127/)|COLING| 2025|NA|
|[Distilling symbolic priors for concept learning into neural networks](https://arxiv.org/abs/2402.07035)| Preprint | 2024| NA|
|[Meta-learning as a bridge between neural networks and symbolic Bayesian models.](https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/metalearning-as-a-bridge-between-neural-networks-and-symbolic-bayesian-models/185DB00366FD4F9B218E36F32886242F)|Behavioral and Brain Sciences| 2024 |[code](https://github.com/marcelbinz/meta-learned-models)|
|[Human-like systematic generalization through a meta-learning neural network](https://www.nature.com/articles/s41586-023-06668-3)|Nature|2023|NA|

# Psychometric
| **Paper**                                                    | **Venue/Journal** | **Year** | **Code**                                               | 
| ------------------------------------------------------------ | --------- | -------- | ------------------------------------------------------------ |
[Language Models Grow Less Humanlike beyond Phase Transition](https://arxiv.org/abs/2502.18802)|Preprint| 2025| NA|
|[Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs](https://arxiv.org/abs/2309.07311)|ICLR|2024| NA|
|[Language Models Grow Less Humanlike beyond Phase Transition](https://arxiv.org/abs/2502.18802)| Preprint| 2025| NA|
|[Linear Recency Bias During Training Improves Transformers’ Fit to Reading Times](https://aclanthology.org/2025.coling-main.517/)| COLING| 2025|[code](https://github.com/christian-clark/recency-bias)|


# Targeted Evaluation
| **Paper**                                                    | **Venue/Journal** | **Year** | **Code**                                               | 
| ------------------------------------------------------------ | --------- | -------- | ------------------------------------------------------------ |
|[Forgetting Transformer: Softmax Attention with a Forget Gate](https://arxiv.org/abs/2503.02130)| ICLR| 2025| [code](https://github.com/zhixuan-lin/forgetting-transformer)|
|[Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs](https://arxiv.org/abs/2309.07311) | ICLR| 2024| NA|
|[Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks](https://arxiv.org/abs/1810.09536)|ICLR| 2019|[code](https://github.com/yikangshen/Ordered-Neurons)|

# Memorization
| **Paper**                                                    | **Venue/Journal** | **Year** | **Code**                                               | 
| ------------------------------------------------------------ | --------- | -------- | ------------------------------------------------------------ |
|[The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization](https://arxiv.org/abs/2110.07732)|  ICLR |2022|[code](https://github.com/robertcsordas/ndr)|


# Others
| **Paper**                                                    | **Venue/Journal** | **Year** | **Code**                                               |
| ------------------------------------------------------------ | --------- | -------- | ------------------------------------------------------------ |
|[Editing Models with Task Arithmetic](https://arxiv.org/abs/2212.04089) | ICLR| 2023| [code](https://github.com/mlfoundations/task_vectors)|
